{
  "judge_model": "ds-v3.2",
  "prompt_version": "v1.0",
  "created_at": "2026-01-28T22:20:01.708242Z",
  "overall": {
    "accuracy": 0.5695,
    "total_evaluated": 9516,
    "evaluated": 9516,
    "correct": 5419,
    "incorrect": 4097,
    "exact_match": {
      "count": 1523,
      "rate": 0.16
    },
    "llm_judged": {
      "count": 7993,
      "rate": 0.84,
      "approved": 3896,
      "rejected": 4097,
      "approval_rate": 0.4874
    },
    "accuracy_from_exact_match": 0.16,
    "accuracy_boost_from_llm": 0.4094,
    "no_llm_fallback_count": 0
  },
  "per_file": [
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 38,
          "rejected": 21,
          "approval_rate": 0.6441
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 32,
          "rejected": 24,
          "approval_rate": 0.5714
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 37,
          "rejected": 20,
          "approval_rate": 0.6491
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 32,
          "rejected": 27,
          "approval_rate": 0.5424
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 29,
          "rejected": 29,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 29,
          "rejected": 32,
          "approval_rate": 0.4754
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 32,
          "rejected": 29,
          "approval_rate": 0.5246
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 33,
          "rejected": 19,
          "approval_rate": 0.6346
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 41,
          "rejected": 16,
          "approval_rate": 0.7193
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 43,
          "rejected": 14,
          "approval_rate": 0.7544
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7049,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 11,
          "rate": 0.1803
        },
        "llm_judged": {
          "count": 50,
          "rate": 0.8197,
          "approved": 38,
          "rejected": 12,
          "approval_rate": 0.76
        },
        "accuracy_from_exact_match": 0.1803,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 11,
        "llm_calls": 50,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 33,
          "rejected": 25,
          "approval_rate": 0.569
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 30,
          "rejected": 28,
          "approval_rate": 0.5172
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4918,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 36,
          "rejected": 22,
          "approval_rate": 0.6207
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 32,
          "rejected": 26,
          "approval_rate": 0.5517
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 40,
          "rejected": 20,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 34,
          "rejected": 17,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 31,
          "rejected": 20,
          "approval_rate": 0.6078
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.5082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 41,
          "rejected": 16,
          "approval_rate": 0.7193
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 40,
          "rejected": 13,
          "approval_rate": 0.7547
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.4426,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 27,
        "incorrect": 34,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 23,
          "rejected": 34,
          "approval_rate": 0.4035
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 16,
          "rejected": 42,
          "approval_rate": 0.2759
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.4262,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 26,
        "incorrect": 35,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 25,
          "rejected": 35,
          "approval_rate": 0.4167
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 26,
          "rejected": 32,
          "approval_rate": 0.4483
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 40,
          "rejected": 16,
          "approval_rate": 0.7143
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 38,
          "rejected": 18,
          "approval_rate": 0.6786
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 40,
          "rejected": 17,
          "approval_rate": 0.7018
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 41,
          "rejected": 18,
          "approval_rate": 0.6949
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 40,
          "rejected": 12,
          "approval_rate": 0.7692
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 37,
          "rejected": 22,
          "approval_rate": 0.6271
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 38,
          "rejected": 21,
          "approval_rate": 0.6441
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 40,
          "rejected": 18,
          "approval_rate": 0.6897
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 41,
          "rejected": 20,
          "approval_rate": 0.6721
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 43,
          "rejected": 16,
          "approval_rate": 0.7288
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.7049,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 39,
          "rejected": 16,
          "approval_rate": 0.7091
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 7,
          "rate": 0.1148
        },
        "llm_judged": {
          "count": 54,
          "rate": 0.8852,
          "approved": 35,
          "rejected": 19,
          "approval_rate": 0.6481
        },
        "accuracy_from_exact_match": 0.1148,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 7,
        "llm_calls": 54,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 41,
          "rejected": 16,
          "approval_rate": 0.7193
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 45,
          "rejected": 15,
          "approval_rate": 0.75
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 44,
          "rejected": 9,
          "approval_rate": 0.8302
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 36,
          "rejected": 24,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 40,
          "rejected": 18,
          "approval_rate": 0.6897
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 40,
          "rejected": 20,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 48,
          "rejected": 12,
          "approval_rate": 0.8
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 38,
          "rejected": 18,
          "approval_rate": 0.6786
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 37,
          "rejected": 19,
          "approval_rate": 0.6607
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 33,
          "rejected": 25,
          "approval_rate": 0.569
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 39,
          "rejected": 14,
          "approval_rate": 0.7358
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 23,
          "rejected": 36,
          "approval_rate": 0.3898
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 20,
          "rejected": 39,
          "approval_rate": 0.339
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 28,
        "incorrect": 33,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 28,
          "rejected": 33,
          "approval_rate": 0.459
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 23,
          "rejected": 38,
          "approval_rate": 0.377
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 28,
          "rejected": 28,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.4426,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 27,
        "incorrect": 34,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 23,
          "rejected": 34,
          "approval_rate": 0.4035
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 29,
          "rejected": 32,
          "approval_rate": 0.4754
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.1639,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 10,
        "incorrect": 51,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 10,
          "rejected": 51,
          "approval_rate": 0.1639
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.1967,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 12,
        "incorrect": 49,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 12,
          "rejected": 49,
          "approval_rate": 0.1967
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.2131,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 13,
        "incorrect": 48,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 12,
          "rejected": 48,
          "approval_rate": 0.2
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.2295,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 14,
        "incorrect": 47,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 14,
          "rejected": 47,
          "approval_rate": 0.2295
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 18,
          "rejected": 17,
          "approval_rate": 0.5143
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 15,
          "rejected": 17,
          "approval_rate": 0.4688
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 18,
          "rejected": 17,
          "approval_rate": 0.5143
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 18,
          "rate": 0.2951
        },
        "llm_judged": {
          "count": 43,
          "rate": 0.7049,
          "approved": 21,
          "rejected": 22,
          "approval_rate": 0.4884
        },
        "accuracy_from_exact_match": 0.2951,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 18,
        "llm_calls": 43,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 17,
          "rejected": 13,
          "approval_rate": 0.5667
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 17,
          "rejected": 18,
          "approval_rate": 0.4857
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 18,
          "rejected": 16,
          "approval_rate": 0.5294
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 20,
          "rejected": 16,
          "approval_rate": 0.5556
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 17,
          "rejected": 23,
          "approval_rate": 0.425
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 15,
          "rejected": 15,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "mistral_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 33,
          "rejected": 25,
          "approval_rate": 0.569
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "mistral_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 32,
          "rejected": 26,
          "approval_rate": 0.5517
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "mistral_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 33,
          "rejected": 27,
          "approval_rate": 0.55
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "mistral_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 36,
          "rejected": 25,
          "approval_rate": 0.5902
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "mistral_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 5,
        "incorrect": 56,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 5,
          "rejected": 56,
          "approval_rate": 0.082
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "mistral_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "mistral_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 22,
          "rejected": 16,
          "approval_rate": 0.5789
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 14,
          "rejected": 15,
          "approval_rate": 0.4828
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 19,
          "rejected": 21,
          "approval_rate": 0.475
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 20,
          "rate": 0.3279
        },
        "llm_judged": {
          "count": 41,
          "rate": 0.6721,
          "approved": 21,
          "rejected": 20,
          "approval_rate": 0.5122
        },
        "accuracy_from_exact_match": 0.3279,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 20,
        "llm_calls": 41,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 13,
          "rejected": 16,
          "approval_rate": 0.4483
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 21,
          "rejected": 16,
          "approval_rate": 0.5676
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 28,
          "rate": 0.459
        },
        "llm_judged": {
          "count": 33,
          "rate": 0.541,
          "approved": 17,
          "rejected": 16,
          "approval_rate": 0.5152
        },
        "accuracy_from_exact_match": 0.459,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 28,
        "llm_calls": 33,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 21,
          "rejected": 21,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 19,
          "rejected": 21,
          "approval_rate": 0.475
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "mistral_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 14,
          "rejected": 15,
          "approval_rate": 0.4828
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "phi3_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.3934,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 24,
        "incorrect": 37,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 23,
          "rejected": 37,
          "approval_rate": 0.3833
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "phi3_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 20,
          "rejected": 38,
          "approval_rate": 0.3448
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "phi3_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.3934,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 24,
        "incorrect": 37,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 23,
          "rejected": 37,
          "approval_rate": 0.3833
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "phi3_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 22,
          "rejected": 36,
          "approval_rate": 0.3793
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "phi3_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "phi3_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "phi3_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 5,
        "incorrect": 56,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 5,
          "rejected": 56,
          "approval_rate": 0.082
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "phi3_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.1148,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 7,
        "incorrect": 54,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 7,
          "rejected": 54,
          "approval_rate": 0.1148
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1148,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 24,
          "rejected": 18,
          "approval_rate": 0.5714
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 17,
          "rejected": 19,
          "approval_rate": 0.4722
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 22,
          "rejected": 16,
          "approval_rate": 0.5789
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 24,
          "rejected": 22,
          "approval_rate": 0.5217
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 18,
          "rejected": 17,
          "approval_rate": 0.5143
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 21,
          "rejected": 19,
          "approval_rate": 0.525
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 21,
          "rejected": 17,
          "approval_rate": 0.5526
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 22,
          "rejected": 15,
          "approval_rate": 0.5946
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 24,
          "rejected": 22,
          "approval_rate": 0.5217
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "phi3_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 16,
          "rejected": 20,
          "approval_rate": 0.4444
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 38,
          "rejected": 19,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 34,
          "rejected": 22,
          "approval_rate": 0.6071
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 43,
          "rejected": 17,
          "approval_rate": 0.7167
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7049,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 42,
          "rejected": 13,
          "approval_rate": 0.7636
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.6885,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.4426,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 27,
        "incorrect": 34,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 25,
          "rejected": 34,
          "approval_rate": 0.4237
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 32,
          "rejected": 27,
          "approval_rate": 0.5424
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 37,
          "rejected": 24,
          "approval_rate": 0.6066
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 36,
          "rejected": 24,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 22,
          "rejected": 39,
          "approval_rate": 0.3607
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 18,
          "rejected": 42,
          "approval_rate": 0.3
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 22,
          "rejected": 38,
          "approval_rate": 0.3667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.0164,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 1,
        "incorrect": 60,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 1,
          "rejected": 60,
          "approval_rate": 0.0164
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0164,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 17,
          "rejected": 17,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 18,
          "rejected": 17,
          "approval_rate": 0.5143
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 24,
          "rejected": 14,
          "approval_rate": 0.6316
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 22,
          "rejected": 20,
          "approval_rate": 0.5238
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 14,
          "rejected": 18,
          "approval_rate": 0.4375
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 22,
          "rate": 0.3607
        },
        "llm_judged": {
          "count": 39,
          "rate": 0.6393,
          "approved": 22,
          "rejected": 17,
          "approval_rate": 0.5641
        },
        "accuracy_from_exact_match": 0.3607,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 22,
        "llm_calls": 39,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 16,
          "rejected": 20,
          "approval_rate": 0.4444
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 24,
          "rejected": 16,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 24,
          "rejected": 22,
          "approval_rate": 0.5217
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 28,
          "rate": 0.459
        },
        "llm_judged": {
          "count": 33,
          "rate": 0.541,
          "approved": 12,
          "rejected": 21,
          "approval_rate": 0.3636
        },
        "accuracy_from_exact_match": 0.459,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 28,
        "llm_calls": 33,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.4918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 30,
        "incorrect": 31,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 25,
          "rejected": 31,
          "approval_rate": 0.4464
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 19,
          "rejected": 38,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 28,
        "incorrect": 33,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 26,
          "rejected": 33,
          "approval_rate": 0.4407
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 22,
          "rejected": 36,
          "approval_rate": 0.3793
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 15,
        "incorrect": 46,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 15,
          "rejected": 46,
          "approval_rate": 0.2459
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.2623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 16,
        "incorrect": 45,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 16,
          "rejected": 45,
          "approval_rate": 0.2623
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 18,
          "rejected": 43,
          "approval_rate": 0.2951
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.2295,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 14,
        "incorrect": 47,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 13,
          "rejected": 47,
          "approval_rate": 0.2167
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 19,
          "rejected": 42,
          "approval_rate": 0.3115
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 19,
          "rejected": 13,
          "approval_rate": 0.5938
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 15,
          "rejected": 14,
          "approval_rate": 0.5172
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 24,
          "rejected": 16,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 19,
          "rejected": 18,
          "approval_rate": 0.5135
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 35,
          "rate": 0.5738
        },
        "llm_judged": {
          "count": 26,
          "rate": 0.4262,
          "approved": 12,
          "rejected": 14,
          "approval_rate": 0.4615
        },
        "accuracy_from_exact_match": 0.5738,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 35,
        "llm_calls": 26,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 14,
          "rejected": 15,
          "approval_rate": 0.4828
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 17,
          "rejected": 18,
          "approval_rate": 0.4857
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 19,
          "rejected": 21,
          "approval_rate": 0.475
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 19,
          "rejected": 19,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 13,
          "rejected": 16,
          "approval_rate": 0.4483
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 37,
          "rejected": 18,
          "approval_rate": 0.6727
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 7,
          "rate": 0.1148
        },
        "llm_judged": {
          "count": 54,
          "rate": 0.8852,
          "approved": 33,
          "rejected": 21,
          "approval_rate": 0.6111
        },
        "accuracy_from_exact_match": 0.1148,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 7,
        "llm_calls": 54,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 37,
          "rejected": 20,
          "approval_rate": 0.6491
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6066,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 40,
          "rejected": 13,
          "approval_rate": 0.7547
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.4426,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 27,
        "incorrect": 34,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 24,
          "rejected": 34,
          "approval_rate": 0.4138
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 27,
          "rejected": 32,
          "approval_rate": 0.4576
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.4426,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.4918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 30,
        "incorrect": 31,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 29,
          "rejected": 31,
          "approval_rate": 0.4833
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 29,
          "rejected": 32,
          "approval_rate": 0.4754
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    }
  ],
  "ranking": [
    {
      "rank": 1,
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.8302,
      "total": 61
    },
    {
      "rank": 2,
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.1803,
      "llm_approval_rate": 0.76,
      "total": 61
    },
    {
      "rank": 3,
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.7692,
      "total": 61
    },
    {
      "rank": 4,
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.8,
      "total": 61
    },
    {
      "rank": 5,
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.7547,
      "total": 61
    },
    {
      "rank": 6,
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.5667,
      "total": 61
    },
    {
      "rank": 7,
      "source_file": "qwen-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.7636,
      "total": 61
    },
    {
      "rank": 8,
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.5938,
      "total": 61
    },
    {
      "rank": 9,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.7547,
      "total": 61
    },
    {
      "rank": 10,
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7544,
      "total": 61
    },
    {
      "rank": 11,
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.7358,
      "total": 61
    },
    {
      "rank": 12,
      "source_file": "qwen-3b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.6316,
      "total": 61
    },
    {
      "rank": 13,
      "source_file": "qwen-7b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.5172,
      "total": 61
    },
    {
      "rank": 14,
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.5738,
      "llm_approval_rate": 0.4615,
      "total": 61
    },
    {
      "rank": 15,
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.75,
      "total": 61
    },
    {
      "rank": 16,
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 17,
      "source_file": "mistral_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4828,
      "total": 61
    },
    {
      "rank": 18,
      "source_file": "mistral_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4828,
      "total": 61
    },
    {
      "rank": 19,
      "source_file": "phi3_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.5946,
      "total": 61
    },
    {
      "rank": 20,
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4828,
      "total": 61
    },
    {
      "rank": 21,
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7193,
      "total": 61
    },
    {
      "rank": 22,
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7193,
      "total": 61
    },
    {
      "rank": 23,
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.7143,
      "total": 61
    },
    {
      "rank": 24,
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.7288,
      "total": 61
    },
    {
      "rank": 25,
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.7091,
      "total": 61
    },
    {
      "rank": 26,
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7193,
      "total": 61
    },
    {
      "rank": 27,
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.5294,
      "total": 61
    },
    {
      "rank": 28,
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.5556,
      "total": 61
    },
    {
      "rank": 29,
      "source_file": "mistral_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.5789,
      "total": 61
    },
    {
      "rank": 30,
      "source_file": "mistral_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4483,
      "total": 61
    },
    {
      "rank": 31,
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.5676,
      "total": 61
    },
    {
      "rank": 32,
      "source_file": "mistral_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.459,
      "llm_approval_rate": 0.5152,
      "total": 61
    },
    {
      "rank": 33,
      "source_file": "phi3_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.5789,
      "total": 61
    },
    {
      "rank": 34,
      "source_file": "qwen-3b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 35,
      "source_file": "qwen-7b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 36,
      "source_file": "qwen-7b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4483,
      "total": 61
    },
    {
      "rank": 37,
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 38,
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7018,
      "total": 61
    },
    {
      "rank": 39,
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.5143,
      "total": 61
    },
    {
      "rank": 40,
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.4688,
      "total": 61
    },
    {
      "rank": 41,
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.5143,
      "total": 61
    },
    {
      "rank": 42,
      "source_file": "phi3_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.5143,
      "total": 61
    },
    {
      "rank": 43,
      "source_file": "phi3_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.5526,
      "total": 61
    },
    {
      "rank": 44,
      "source_file": "qwen-32b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.7167,
      "total": 61
    },
    {
      "rank": 45,
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 46,
      "source_file": "qwen-3b_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.5143,
      "total": 61
    },
    {
      "rank": 47,
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.3607,
      "llm_approval_rate": 0.5641,
      "total": 61
    },
    {
      "rank": 48,
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.6786,
      "total": 61
    },
    {
      "rank": 49,
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6949,
      "total": 61
    },
    {
      "rank": 50,
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6897,
      "total": 61
    },
    {
      "rank": 51,
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6897,
      "total": 61
    },
    {
      "rank": 52,
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.6786,
      "total": 61
    },
    {
      "rank": 53,
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.4857,
      "total": 61
    },
    {
      "rank": 54,
      "source_file": "phi3_finetuned_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.5714,
      "total": 61
    },
    {
      "rank": 55,
      "source_file": "qwen-3b_finetuned_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.4375,
      "total": 61
    },
    {
      "rank": 56,
      "source_file": "qwen-7b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.5135,
      "total": 61
    },
    {
      "rank": 57,
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.4857,
      "total": 61
    },
    {
      "rank": 58,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.6727,
      "total": 61
    },
    {
      "rank": 59,
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.6346,
      "total": 61
    },
    {
      "rank": 60,
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.1148,
      "llm_approval_rate": 0.6481,
      "total": 61
    },
    {
      "rank": 61,
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.6607,
      "total": 61
    },
    {
      "rank": 62,
      "source_file": "phi3_finetuned_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.4722,
      "total": 61
    },
    {
      "rank": 63,
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.525,
      "total": 61
    },
    {
      "rank": 64,
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 65,
      "source_file": "qwen-7b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 66,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.6491,
      "total": 61
    },
    {
      "rank": 67,
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 68,
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.6078,
      "total": 61
    },
    {
      "rank": 69,
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.6721,
      "total": 61
    },
    {
      "rank": 70,
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 71,
      "source_file": "mistral_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.3279,
      "llm_approval_rate": 0.5122,
      "total": 61
    },
    {
      "rank": 72,
      "source_file": "phi3_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.4444,
      "total": 61
    },
    {
      "rank": 73,
      "source_file": "qwen-3b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.5238,
      "total": 61
    },
    {
      "rank": 74,
      "source_file": "qwen-3b_finetuned_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.4444,
      "total": 61
    },
    {
      "rank": 75,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.6491,
      "total": 61
    },
    {
      "rank": 76,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6441,
      "total": 61
    },
    {
      "rank": 77,
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6441,
      "total": 61
    },
    {
      "rank": 78,
      "source_file": "mistral_finetuned_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.475,
      "total": 61
    },
    {
      "rank": 79,
      "source_file": "mistral_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 80,
      "source_file": "mistral_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.475,
      "total": 61
    },
    {
      "rank": 81,
      "source_file": "qwen-3b_finetuned_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.459,
      "llm_approval_rate": 0.3636,
      "total": 61
    },
    {
      "rank": 82,
      "source_file": "qwen-7b_finetuned_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.475,
      "total": 61
    },
    {
      "rank": 83,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.1148,
      "llm_approval_rate": 0.6111,
      "total": 61
    },
    {
      "rank": 84,
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6207,
      "total": 61
    },
    {
      "rank": 85,
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6271,
      "total": 61
    },
    {
      "rank": 86,
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.2951,
      "llm_approval_rate": 0.4884,
      "total": 61
    },
    {
      "rank": 87,
      "source_file": "phi3_finetuned_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.5217,
      "total": 61
    },
    {
      "rank": 88,
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.5217,
      "total": 61
    },
    {
      "rank": 89,
      "source_file": "qwen-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.6071,
      "total": 61
    },
    {
      "rank": 90,
      "source_file": "qwen-3b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.5217,
      "total": 61
    },
    {
      "rank": 91,
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.425,
      "total": 61
    },
    {
      "rank": 92,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5714,
      "total": 61
    },
    {
      "rank": 93,
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 94,
      "source_file": "qwen-32b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.6066,
      "total": 61
    },
    {
      "rank": 95,
      "source_file": "qwen-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 96,
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.569,
      "total": 61
    },
    {
      "rank": 97,
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.569,
      "total": 61
    },
    {
      "rank": 98,
      "source_file": "mistral_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.569,
      "total": 61
    },
    {
      "rank": 99,
      "source_file": "mistral_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.5902,
      "total": 61
    },
    {
      "rank": 100,
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5517,
      "total": 61
    },
    {
      "rank": 101,
      "source_file": "mistral_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5517,
      "total": 61
    },
    {
      "rank": 102,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.5424,
      "total": 61
    },
    {
      "rank": 103,
      "source_file": "mistral_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.55,
      "total": 61
    },
    {
      "rank": 104,
      "source_file": "qwen-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.5424,
      "total": 61
    },
    {
      "rank": 105,
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5172,
      "total": 61
    },
    {
      "rank": 106,
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 107,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 108,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.5246,
      "total": 61
    },
    {
      "rank": 109,
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.4918,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.4464,
      "total": 61
    },
    {
      "rank": 110,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.4918,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.4833,
      "total": 61
    },
    {
      "rank": 111,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.4754,
      "total": 61
    },
    {
      "rank": 112,
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.4483,
      "total": 61
    },
    {
      "rank": 113,
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.4754,
      "total": 61
    },
    {
      "rank": 114,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4576,
      "total": 61
    },
    {
      "rank": 115,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.4754,
      "total": 61
    },
    {
      "rank": 116,
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.459,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.459,
      "total": 61
    },
    {
      "rank": 117,
      "source_file": "qwen-7b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.459,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4407,
      "total": 61
    },
    {
      "rank": 118,
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.4426,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.4035,
      "total": 61
    },
    {
      "rank": 119,
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.4426,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.4035,
      "total": 61
    },
    {
      "rank": 120,
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.4426,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4237,
      "total": 61
    },
    {
      "rank": 121,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.4426,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.4138,
      "total": 61
    },
    {
      "rank": 122,
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.4262,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.4167,
      "total": 61
    },
    {
      "rank": 123,
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.3898,
      "total": 61
    },
    {
      "rank": 124,
      "source_file": "phi3_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3793,
      "total": 61
    },
    {
      "rank": 125,
      "source_file": "qwen-7b_baseline_few_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3793,
      "total": 61
    },
    {
      "rank": 126,
      "source_file": "phi3_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.3934,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3833,
      "total": 61
    },
    {
      "rank": 127,
      "source_file": "phi3_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.3934,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3833,
      "total": 61
    },
    {
      "rank": 128,
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.377,
      "total": 61
    },
    {
      "rank": 129,
      "source_file": "phi3_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3448,
      "total": 61
    },
    {
      "rank": 130,
      "source_file": "qwen-3b_baseline_few_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3667,
      "total": 61
    },
    {
      "rank": 131,
      "source_file": "qwen-7b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 132,
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.339,
      "total": 61
    },
    {
      "rank": 133,
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3607,
      "total": 61
    },
    {
      "rank": 134,
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.2759,
      "total": 61
    },
    {
      "rank": 135,
      "source_file": "qwen-3b_baseline_few_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3,
      "total": 61
    },
    {
      "rank": 136,
      "source_file": "qwen-7b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3115,
      "total": 61
    },
    {
      "rank": 137,
      "source_file": "qwen-7b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2951,
      "total": 61
    },
    {
      "rank": 138,
      "source_file": "qwen-7b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.2623,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2623,
      "total": 61
    },
    {
      "rank": 139,
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.2459,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2459,
      "total": 61
    },
    {
      "rank": 140,
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.2295,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2295,
      "total": 61
    },
    {
      "rank": 141,
      "source_file": "qwen-7b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.2295,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.2167,
      "total": 61
    },
    {
      "rank": 142,
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.2131,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.2,
      "total": 61
    },
    {
      "rank": 143,
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.1967,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1967,
      "total": 61
    },
    {
      "rank": 144,
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.1639,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1639,
      "total": 61
    },
    {
      "rank": 145,
      "source_file": "phi3_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.1148,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1148,
      "total": 61
    },
    {
      "rank": 146,
      "source_file": "mistral_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.082,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.082,
      "total": 61
    },
    {
      "rank": 147,
      "source_file": "phi3_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.082,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.082,
      "total": 61
    },
    {
      "rank": 148,
      "source_file": "phi3_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 149,
      "source_file": "phi3_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 150,
      "source_file": "mistral_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 151,
      "source_file": "qwen-3b_baseline_zero_shot_seed45__judge-ds-v3.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 152,
      "source_file": "mistral_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 153,
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-ds-v3.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 154,
      "source_file": "qwen-3b_baseline_zero_shot_seed43__judge-ds-v3.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 155,
      "source_file": "qwen-3b_baseline_zero_shot_seed44__judge-ds-v3.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 156,
      "source_file": "qwen-3b_baseline_zero_shot_seed46__judge-ds-v3.2.json",
      "accuracy": 0.0164,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0164,
      "total": 61
    }
  ],
  "totals": {
    "evaluated": 9516,
    "auto_exact": 1523,
    "llm_calls": 7993,
    "no_llm": 0
  }
}