{
  "judge_model": "llama-70b",
  "prompt_version": "v1.0",
  "created_at": "2026-01-28T22:20:01.879140Z",
  "overall": {
    "accuracy": 0.7726,
    "total_evaluated": 5795,
    "evaluated": 5795,
    "correct": 4477,
    "incorrect": 1318,
    "exact_match": {
      "count": 762,
      "rate": 0.1315
    },
    "llm_judged": {
      "count": 5033,
      "rate": 0.8685,
      "approved": 3715,
      "rejected": 1318,
      "approval_rate": 0.7381
    },
    "accuracy_from_exact_match": 0.1315,
    "accuracy_boost_from_llm": 0.6411,
    "no_llm_fallback_count": 0
  },
  "per_file": [
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 50,
          "rejected": 9,
          "approval_rate": 0.8475
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 44,
          "rejected": 12,
          "approval_rate": 0.7857
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 49,
          "rejected": 8,
          "approval_rate": 0.8596
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 48,
          "rejected": 11,
          "approval_rate": 0.8136
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 44,
          "rejected": 14,
          "approval_rate": 0.7586
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 48,
          "rejected": 13,
          "approval_rate": 0.7869
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 50,
          "rejected": 11,
          "approval_rate": 0.8197
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 41,
          "rejected": 11,
          "approval_rate": 0.7885
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 47,
          "rejected": 10,
          "approval_rate": 0.8246
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 50,
          "rejected": 7,
          "approval_rate": 0.8772
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.9016,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 55,
        "incorrect": 6,
        "exact_match": {
          "count": 11,
          "rate": 0.1803
        },
        "llm_judged": {
          "count": 50,
          "rate": 0.8197,
          "approved": 44,
          "rejected": 6,
          "approval_rate": 0.88
        },
        "accuracy_from_exact_match": 0.1803,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 11,
        "llm_calls": 50,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 46,
          "rejected": 12,
          "approval_rate": 0.7931
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 44,
          "rejected": 14,
          "approval_rate": 0.7586
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 50,
          "rejected": 8,
          "approval_rate": 0.8621
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 46,
          "rejected": 12,
          "approval_rate": 0.7931
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 53,
          "rejected": 7,
          "approval_rate": 0.8833
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.8689,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 39,
          "rejected": 12,
          "approval_rate": 0.7647
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 39,
          "rejected": 12,
          "approval_rate": 0.7647
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 50,
          "rejected": 7,
          "approval_rate": 0.8772
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 56,
        "incorrect": 5,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 48,
          "rejected": 5,
          "approval_rate": 0.9057
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 45,
          "rejected": 12,
          "approval_rate": 0.7895
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 46,
          "rejected": 12,
          "approval_rate": 0.7931
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 44,
          "rejected": 16,
          "approval_rate": 0.7333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 46,
          "rejected": 12,
          "approval_rate": 0.7931
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 47,
          "rejected": 9,
          "approval_rate": 0.8393
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 44,
          "rejected": 12,
          "approval_rate": 0.7857
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 48,
          "rejected": 9,
          "approval_rate": 0.8421
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 56,
        "incorrect": 5,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 54,
          "rejected": 5,
          "approval_rate": 0.9153
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.8852,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.9016,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 55,
        "incorrect": 6,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 46,
          "rejected": 6,
          "approval_rate": 0.8846
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 51,
          "rejected": 8,
          "approval_rate": 0.8644
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.8361,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 46,
          "rejected": 13,
          "approval_rate": 0.7797
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 50,
          "rejected": 8,
          "approval_rate": 0.8621
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.9016,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 55,
        "incorrect": 6,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 55,
          "rejected": 6,
          "approval_rate": 0.9016
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.9016,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 49,
          "rejected": 10,
          "approval_rate": 0.8305
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 46,
          "rejected": 9,
          "approval_rate": 0.8364
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 7,
          "rate": 0.1148
        },
        "llm_judged": {
          "count": 54,
          "rate": 0.8852,
          "approved": 43,
          "rejected": 11,
          "approval_rate": 0.7963
        },
        "accuracy_from_exact_match": 0.1148,
        "accuracy_boost_from_llm": 0.7049,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 7,
        "llm_calls": 54,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 47,
          "rejected": 10,
          "approval_rate": 0.8246
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.9016,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 55,
        "incorrect": 6,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 54,
          "rejected": 6,
          "approval_rate": 0.9
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.8852,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 56,
        "incorrect": 5,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 48,
          "rejected": 5,
          "approval_rate": 0.9057
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 45,
          "rejected": 15,
          "approval_rate": 0.75
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 50,
          "rejected": 8,
          "approval_rate": 0.8621
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 50,
          "rejected": 10,
          "approval_rate": 0.8333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 50,
          "rejected": 10,
          "approval_rate": 0.8333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.8197,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "llama-70b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 49,
          "rejected": 7,
          "approval_rate": 0.875
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-llama-70b.json",
      "stem": "llama-70b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.8689,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 53,
        "incorrect": 8,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 48,
          "rejected": 8,
          "approval_rate": 0.8571
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "llama-70b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.8525,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 52,
        "incorrect": 9,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 49,
          "rejected": 9,
          "approval_rate": 0.8448
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-llama-70b.json",
      "stem": "llama-70b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 46,
          "rejected": 7,
          "approval_rate": 0.8679
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "llama-70b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 40,
          "rejected": 19,
          "approval_rate": 0.678
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.6557,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "llama-70b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 41,
          "rejected": 18,
          "approval_rate": 0.6949
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-llama-70b.json",
      "stem": "llama-70b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 49,
          "rejected": 12,
          "approval_rate": 0.8033
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "llama-70b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 47,
          "rejected": 14,
          "approval_rate": 0.7705
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "llama-8b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 41,
          "rejected": 15,
          "approval_rate": 0.7321
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "llama-8b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 42,
          "rejected": 15,
          "approval_rate": 0.7368
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.6885,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "llama-8b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 41,
          "rejected": 20,
          "approval_rate": 0.6721
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.6721,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "llama-8b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.3934,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 24,
        "incorrect": 37,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 24,
          "rejected": 37,
          "approval_rate": 0.3934
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "llama-8b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.4262,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 26,
        "incorrect": 35,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 26,
          "rejected": 35,
          "approval_rate": 0.4262
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-llama-70b.json",
      "stem": "llama-8b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.4262,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 26,
        "incorrect": 35,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 25,
          "rejected": 35,
          "approval_rate": 0.4167
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "stem": "llama-8b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 33,
          "rejected": 28,
          "approval_rate": 0.541
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 22,
          "rejected": 13,
          "approval_rate": 0.6286
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 21,
          "rejected": 11,
          "approval_rate": 0.6562
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 24,
          "rejected": 11,
          "approval_rate": 0.6857
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 18,
          "rate": 0.2951
        },
        "llm_judged": {
          "count": 43,
          "rate": 0.7049,
          "approved": 29,
          "rejected": 14,
          "approval_rate": 0.6744
        },
        "accuracy_from_exact_match": 0.2951,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 18,
        "llm_calls": 43,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 18,
          "rejected": 12,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 23,
          "rejected": 12,
          "approval_rate": 0.6571
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 22,
          "rejected": 12,
          "approval_rate": 0.6471
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 25,
          "rejected": 11,
          "approval_rate": 0.6944
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 26,
          "rejected": 14,
          "approval_rate": 0.65
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-llama-70b.json",
      "stem": "llama-8b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 19,
          "rejected": 11,
          "approval_rate": 0.6333
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "mistral_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 44,
          "rejected": 14,
          "approval_rate": 0.7586
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.7213,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed43__judge-llama-70b.json",
      "stem": "mistral_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 42,
          "rejected": 16,
          "approval_rate": 0.7241
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.6885,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed44__judge-llama-70b.json",
      "stem": "mistral_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 46,
          "rejected": 14,
          "approval_rate": 0.7667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed45__judge-llama-70b.json",
      "stem": "mistral_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 49,
          "rejected": 12,
          "approval_rate": 0.8033
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.8033,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "mistral_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2131,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 13,
        "incorrect": 48,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 13,
          "rejected": 48,
          "approval_rate": 0.2131
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed43__judge-llama-70b.json",
      "stem": "mistral_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.2459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 15,
        "incorrect": 46,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 15,
          "rejected": 46,
          "approval_rate": 0.2459
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed42__judge-llama-70b.json",
      "stem": "mistral_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 26,
          "rejected": 12,
          "approval_rate": 0.6842
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "stem": "mistral_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 25,
          "rejected": 12,
          "approval_rate": 0.6757
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "phi3_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 46,
          "rejected": 14,
          "approval_rate": 0.7667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "phi3_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2295,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 14,
        "incorrect": 47,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 14,
          "rejected": 47,
          "approval_rate": 0.2295
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed42__judge-llama-70b.json",
      "stem": "phi3_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 28,
          "rejected": 14,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "stem": "phi3_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 29,
          "rejected": 11,
          "approval_rate": 0.725
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-llama-70b.json",
      "stem": "phi3_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 34,
          "rejected": 12,
          "approval_rate": 0.7391
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8361,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 51,
        "incorrect": 10,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 47,
          "rejected": 10,
          "approval_rate": 0.8246
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 47,
          "rejected": 12,
          "approval_rate": 0.7966
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.7705,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-3b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 35,
          "rejected": 26,
          "approval_rate": 0.5738
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-3b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 19,
          "rejected": 42,
          "approval_rate": 0.3115
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-3b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7869,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 48,
        "incorrect": 13,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 21,
          "rejected": 13,
          "approval_rate": 0.6176
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 22,
          "rate": 0.3607
        },
        "llm_judged": {
          "count": 39,
          "rate": 0.6393,
          "approved": 27,
          "rejected": 12,
          "approval_rate": 0.6923
        },
        "accuracy_from_exact_match": 0.3607,
        "accuracy_boost_from_llm": 0.4426,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 22,
        "llm_calls": 39,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-7b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 42,
          "rejected": 14,
          "approval_rate": 0.75
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.6885,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-7b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 43,
          "rejected": 18,
          "approval_rate": 0.7049
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.7049,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-7b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 20,
          "rejected": 12,
          "approval_rate": 0.625
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-llama-70b.json",
      "stem": "qwen-7b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8033,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 49,
        "incorrect": 12,
        "exact_match": {
          "count": 35,
          "rate": 0.5738
        },
        "llm_judged": {
          "count": 26,
          "rate": 0.4262,
          "approved": 14,
          "rejected": 12,
          "approval_rate": 0.5385
        },
        "accuracy_from_exact_match": 0.5738,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 35,
        "llm_calls": 26,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 18,
          "rejected": 11,
          "approval_rate": 0.6207
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-llama-70b.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.7541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 46,
        "incorrect": 15,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 20,
          "rejected": 15,
          "approval_rate": 0.5714
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.8852,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 54,
        "incorrect": 7,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 48,
          "rejected": 7,
          "approval_rate": 0.8727
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.7869,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 39,
          "rejected": 19,
          "approval_rate": 0.6724
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    }
  ],
  "ranking": [
    {
      "rank": 1,
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.918,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.9057,
      "total": 61
    },
    {
      "rank": 2,
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.918,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.9153,
      "total": 61
    },
    {
      "rank": 3,
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.918,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.9057,
      "total": 61
    },
    {
      "rank": 4,
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.9016,
      "exact_match_rate": 0.1803,
      "llm_approval_rate": 0.88,
      "total": 61
    },
    {
      "rank": 5,
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.9016,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.8846,
      "total": 61
    },
    {
      "rank": 6,
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.9016,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.9016,
      "total": 61
    },
    {
      "rank": 7,
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.9016,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.9,
      "total": 61
    },
    {
      "rank": 8,
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8772,
      "total": 61
    },
    {
      "rank": 9,
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.8833,
      "total": 61
    },
    {
      "rank": 10,
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8772,
      "total": 61
    },
    {
      "rank": 11,
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.875,
      "total": 61
    },
    {
      "rank": 12,
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.8679,
      "total": 61
    },
    {
      "rank": 13,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8852,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.8727,
      "total": 61
    },
    {
      "rank": 14,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8596,
      "total": 61
    },
    {
      "rank": 15,
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.8621,
      "total": 61
    },
    {
      "rank": 16,
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.8644,
      "total": 61
    },
    {
      "rank": 17,
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.8621,
      "total": 61
    },
    {
      "rank": 18,
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.8621,
      "total": 61
    },
    {
      "rank": 19,
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8689,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.8571,
      "total": 61
    },
    {
      "rank": 20,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.8475,
      "total": 61
    },
    {
      "rank": 21,
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.8393,
      "total": 61
    },
    {
      "rank": 22,
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8421,
      "total": 61
    },
    {
      "rank": 23,
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.8364,
      "total": 61
    },
    {
      "rank": 24,
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8525,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.8448,
      "total": 61
    },
    {
      "rank": 25,
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8246,
      "total": 61
    },
    {
      "rank": 26,
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.8305,
      "total": 61
    },
    {
      "rank": 27,
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8246,
      "total": 61
    },
    {
      "rank": 28,
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.8333,
      "total": 61
    },
    {
      "rank": 29,
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.8333,
      "total": 61
    },
    {
      "rank": 30,
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8361,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.8246,
      "total": 61
    },
    {
      "rank": 31,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.8136,
      "total": 61
    },
    {
      "rank": 32,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.8197,
      "total": 61
    },
    {
      "rank": 33,
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.7885,
      "total": 61
    },
    {
      "rank": 34,
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.1148,
      "llm_approval_rate": 0.7963,
      "total": 61
    },
    {
      "rank": 35,
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.6562,
      "total": 61
    },
    {
      "rank": 36,
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.6857,
      "total": 61
    },
    {
      "rank": 37,
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.6944,
      "total": 61
    },
    {
      "rank": 38,
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.6333,
      "total": 61
    },
    {
      "rank": 39,
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.725,
      "total": 61
    },
    {
      "rank": 40,
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.6207,
      "total": 61
    },
    {
      "rank": 41,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.7857,
      "total": 61
    },
    {
      "rank": 42,
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7931,
      "total": 61
    },
    {
      "rank": 43,
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7931,
      "total": 61
    },
    {
      "rank": 44,
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.7647,
      "total": 61
    },
    {
      "rank": 45,
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.7647,
      "total": 61
    },
    {
      "rank": 46,
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7895,
      "total": 61
    },
    {
      "rank": 47,
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7931,
      "total": 61
    },
    {
      "rank": 48,
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7931,
      "total": 61
    },
    {
      "rank": 49,
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.7857,
      "total": 61
    },
    {
      "rank": 50,
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.8033,
      "total": 61
    },
    {
      "rank": 51,
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 52,
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.6571,
      "total": 61
    },
    {
      "rank": 53,
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.6471,
      "total": 61
    },
    {
      "rank": 54,
      "source_file": "mistral_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.8033,
      "total": 61
    },
    {
      "rank": 55,
      "source_file": "mistral_finetuned_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.6842,
      "total": 61
    },
    {
      "rank": 56,
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.6757,
      "total": 61
    },
    {
      "rank": 57,
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.7391,
      "total": 61
    },
    {
      "rank": 58,
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.7966,
      "total": 61
    },
    {
      "rank": 59,
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.3607,
      "llm_approval_rate": 0.6923,
      "total": 61
    },
    {
      "rank": 60,
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.625,
      "total": 61
    },
    {
      "rank": 61,
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.8033,
      "exact_match_rate": 0.5738,
      "llm_approval_rate": 0.5385,
      "total": 61
    },
    {
      "rank": 62,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.7869,
      "total": 61
    },
    {
      "rank": 63,
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.7797,
      "total": 61
    },
    {
      "rank": 64,
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.6286,
      "total": 61
    },
    {
      "rank": 65,
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7869,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.6176,
      "total": 61
    },
    {
      "rank": 66,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7586,
      "total": 61
    },
    {
      "rank": 67,
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7586,
      "total": 61
    },
    {
      "rank": 68,
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.7705,
      "total": 61
    },
    {
      "rank": 69,
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.2951,
      "llm_approval_rate": 0.6744,
      "total": 61
    },
    {
      "rank": 70,
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.65,
      "total": 61
    },
    {
      "rank": 71,
      "source_file": "mistral_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7586,
      "total": 61
    },
    {
      "rank": 72,
      "source_file": "mistral_baseline_few_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.7667,
      "total": 61
    },
    {
      "rank": 73,
      "source_file": "phi3_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.7667,
      "total": 61
    },
    {
      "rank": 74,
      "source_file": "phi3_finetuned_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 75,
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.75,
      "total": 61
    },
    {
      "rank": 76,
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.75,
      "total": 61
    },
    {
      "rank": 77,
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.7321,
      "total": 61
    },
    {
      "rank": 78,
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.7368,
      "total": 61
    },
    {
      "rank": 79,
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7541,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.5714,
      "total": 61
    },
    {
      "rank": 80,
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-llama-70b.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.7333,
      "total": 61
    },
    {
      "rank": 81,
      "source_file": "mistral_baseline_few_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.7241,
      "total": 61
    },
    {
      "rank": 82,
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6949,
      "total": 61
    },
    {
      "rank": 83,
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.7049,
      "total": 61
    },
    {
      "rank": 84,
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.678,
      "total": 61
    },
    {
      "rank": 85,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6724,
      "total": 61
    },
    {
      "rank": 86,
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.6721,
      "total": 61
    },
    {
      "rank": 87,
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.5738,
      "total": 61
    },
    {
      "rank": 88,
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-llama-70b.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.541,
      "total": 61
    },
    {
      "rank": 89,
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.4262,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.4262,
      "total": 61
    },
    {
      "rank": 90,
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-llama-70b.json",
      "accuracy": 0.4262,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.4167,
      "total": 61
    },
    {
      "rank": 91,
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.3934,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3934,
      "total": 61
    },
    {
      "rank": 92,
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3115,
      "total": 61
    },
    {
      "rank": 93,
      "source_file": "mistral_baseline_zero_shot_seed43__judge-llama-70b.json",
      "accuracy": 0.2459,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2459,
      "total": 61
    },
    {
      "rank": 94,
      "source_file": "phi3_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.2295,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2295,
      "total": 61
    },
    {
      "rank": 95,
      "source_file": "mistral_baseline_zero_shot_seed42__judge-llama-70b.json",
      "accuracy": 0.2131,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2131,
      "total": 61
    }
  ],
  "totals": {
    "evaluated": 5795,
    "auto_exact": 762,
    "llm_calls": 5033,
    "no_llm": 0
  }
}