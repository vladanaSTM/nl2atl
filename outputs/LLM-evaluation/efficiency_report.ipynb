{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Efficiency Report\n",
        "Report file: `efficiency_report.json`\n",
        "\n",
        "This notebook explores model efficiency metrics including accuracy, latency, and derived efficiency scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "report_path = Path(\"outputs/LLM-evaluation/efficiency_report.json\")\n",
        "if not report_path.exists():\n",
        "    report_path = Path(\"efficiency_report.json\")\n",
        "\n",
        "report = json.loads(report_path.read_text())\n",
        "df = pd.DataFrame(report['models'])\n",
        "print(f\"Loaded {len(df)} model-condition entries\")\n",
        "print(f\"Created: {report['created_at']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top Models by Efficiency Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = ['model_short', 'condition', 'accuracy', 'latency_mean_ms', 'efficiency_score', 'confidence_score']\n",
        "display(df[[c for c in cols if c in df.columns]].sort_values('efficiency_score', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top Most Accurate Models (with Judge Confidence Scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show top accurate models with confidence scores\n",
        "acc_cols = ['model_short', 'condition', 'accuracy', 'confidence_score', 'num_seeds', 'latency_mean_ms']\n",
        "top_accurate = df[[c for c in acc_cols if c in df.columns]].sort_values('accuracy', ascending=False).head(10)\n",
        "print('\\n=== Top 10 Most Accurate Models (with Judge Agreement Confidence) ===')\n",
        "display(top_accurate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy by Model and Condition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "df_sorted = df.sort_values('accuracy', ascending=False)\n",
        "labels = df_sorted['model_short'] + ' (' + df_sorted['condition'].fillna('') + ')'\n",
        "colors = ['green' if x else 'blue' for x in df_sorted['finetuned']]\n",
        "plt.barh(range(len(df_sorted)), df_sorted['accuracy'], color=colors)\n",
        "plt.yticks(range(len(df_sorted)), labels, fontsize=8)\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Accuracy by Model-Condition (Green=Finetuned, Blue=Baseline)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Latency Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "df_sorted = df.sort_values('latency_mean_ms', ascending=True)\n",
        "labels = df_sorted['model_short'] + ' (' + df_sorted['condition'].fillna('') + ')'\n",
        "plt.barh(range(len(df_sorted)), df_sorted['latency_mean_ms'])\n",
        "plt.yticks(range(len(df_sorted)), labels, fontsize=8)\n",
        "plt.xlabel('Mean Latency (ms)')\n",
        "plt.title('Latency by Model-Condition (Lower is Better)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Efficiency Score Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "df_sorted = df.sort_values('efficiency_score', ascending=False)\n",
        "labels = df_sorted['model_short'] + ' (' + df_sorted['condition'].fillna('') + ')'\n",
        "plt.barh(range(len(df_sorted)), df_sorted['efficiency_score'])\n",
        "plt.yticks(range(len(df_sorted)), labels, fontsize=8)\n",
        "plt.xlabel('Efficiency Score')\n",
        "plt.title('Composite Efficiency Score (Higher is Better)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy vs Latency Scatter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if x else 'blue' for x in df['finetuned']]\n",
        "plt.scatter(df['latency_mean_ms'], df['accuracy'], c=colors, alpha=0.7, s=100)\n",
        "for idx, row in df.iterrows():\n",
        "    plt.annotate(row['model_short'], (row['latency_mean_ms'], row['accuracy']), fontsize=7)\n",
        "plt.xlabel('Mean Latency (ms)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Latency (Green=Finetuned, Blue=Baseline)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary by Condition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "by_condition = pd.DataFrame(report['by_condition']).T\n",
        "display(by_condition)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary by Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "by_model = pd.DataFrame(report['by_model']).T\n",
        "display(by_model.sort_values('accuracy_best', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rankings Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ranking_name, ranking_data in report['rankings'].items():\n",
        "    print(f\"\\n=== {ranking_name.upper()} ===\")\n",
        "    for item in ranking_data:\n",
        "        print(f\"  {item['rank']}. {item['model']} ({item.get('condition', 'N/A')})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}