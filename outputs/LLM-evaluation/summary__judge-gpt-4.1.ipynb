{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ATL LLM Judge Summary\n",
        "Summary file: summary__judge-gpt-4.1.json\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "summary_path = Path(\"summary__judge-gpt-4.1.json\")\n",
        "if not summary_path.exists():\n",
        "    for parent in Path.cwd().parents:\n",
        "        candidate = parent / summary_path.name\n",
        "        if candidate.exists():\n",
        "            summary_path = candidate\n",
        "            break\n",
        "\n",
        "summary = json.loads(summary_path.read_text())\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        'source_file': item['source_file'],\n",
        "        'accuracy': item['metrics']['accuracy'],\n",
        "        'evaluated': item['metrics']['total_evaluated'],\n",
        "        'correct': item['metrics']['correct'],\n",
        "    }\n",
        "    for item in summary['per_file']\n",
        "])\n",
        "df.sort_values('accuracy', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(100, 40))\n",
        "plt.bar(df['source_file'], df['accuracy'])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy by Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}