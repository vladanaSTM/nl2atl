{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Aggregated Seed Metrics\n",
        "Generated from `outputs/LLM-evaluation/seed_aggregate_metrics_from_judged.json`.\n",
        "This notebook loads the aggregated JSON and shows comparison tables and plots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Requirements\n",
        "Install the plotting and data libraries if needed:\n",
        "```\n",
        "pip install pandas matplotlib seaborn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # seaborn should be installed in the notebook kernel\n",
        "from IPython.display import display\n",
        "# Use POSIX-style path to avoid Windows backslash escape warnings\n",
        "PATH = '/home/infres/vperlic/projects/nl2atl/outputs/LLM-evaluation/seed_aggregate_metrics_from_judged.json'\n",
        "with open(PATH, 'r', encoding='utf-8') as fh:\n",
        "    aggregates = json.load(fh)\n",
        "rows = []\n",
        "for g in aggregates:\n",
        "    row = {k: g.get(k) for k in ('model_short','condition','finetuned','few_shot','num_seeds')}\n",
        "    for metric,vals in (g.get('metrics') or {}).items():\n",
        "        row[f'{metric}_mean'] = vals.get('mean')\n",
        "        row[f'{metric}_std'] = vals.get('std')\n",
        "    # Add confidence score (mean if aggregated)\n",
        "    judge_agreement = g.get('judge_agreement', {})\n",
        "    conf_score = judge_agreement.get('confidence_score')\n",
        "    if isinstance(conf_score, dict):\n",
        "        row['confidence_score'] = conf_score.get('mean')\n",
        "    else:\n",
        "        row['confidence_score'] = conf_score\n",
        "    rows.append(row)\n",
        "df = pd.DataFrame(rows)\n",
        "display(df.sort_values(by=['model_short','condition']).head(20))\n",
        "# Show top models by accuracy with confidence scores\n",
        "if 'accuracy_mean' in df.columns:\n",
        "    top_models = df.sort_values('accuracy_mean', ascending=False).head(10)\n",
        "    display_cols = ['model_short', 'condition', 'accuracy_mean', 'confidence_score', 'num_seeds']\n",
        "    print('\\n=== Top 10 Most Accurate Models (with Confidence Scores) ===')\n",
        "    display(top_models[[c for c in display_cols if c in top_models.columns]])\n",
        "    plot_df = top_models\n",
        "    plt.figure(figsize=(12,6))\n",
        "    sns.barplot(data=plot_df, x='accuracy_mean', y='model_short', hue='condition', dodge=False)\n",
        "    plt.title('Top 10 Groups by Accuracy (with Judge Agreement Scores)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Display aggregated table, ranking by accuracy_mean when available\n",
        "if 'accuracy_mean' in df.columns:\n",
        "    display(df.sort_values(by='accuracy_mean', ascending=False).head(20))\n",
        "else:\n",
        "    display(df.sort_values(by=['model_short','condition']).head(20))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}