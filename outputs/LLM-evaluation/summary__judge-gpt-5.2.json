{
  "judge_model": "gpt-5.2",
  "prompt_version": "v1.0",
  "created_at": "2026-01-28T22:20:01.838832Z",
  "overall": {
    "accuracy": 0.4757,
    "total_evaluated": 9516,
    "evaluated": 9516,
    "correct": 4527,
    "incorrect": 4989,
    "exact_match": {
      "count": 1523,
      "rate": 0.16
    },
    "llm_judged": {
      "count": 7993,
      "rate": 0.84,
      "approved": 3004,
      "rejected": 4989,
      "approval_rate": 0.3758
    },
    "accuracy_from_exact_match": 0.16,
    "accuracy_boost_from_llm": 0.3157,
    "no_llm_fallback_count": 0
  },
  "per_file": [
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.5082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 31,
        "incorrect": 30,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 29,
          "rejected": 30,
          "approval_rate": 0.4915
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 31,
        "incorrect": 30,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 26,
          "rejected": 30,
          "approval_rate": 0.4643
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 32,
          "rejected": 25,
          "approval_rate": 0.5614
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2787,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 17,
        "incorrect": 44,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 15,
          "rejected": 44,
          "approval_rate": 0.2542
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 19,
          "rejected": 39,
          "approval_rate": 0.3276
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3115,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.2295,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 14,
        "incorrect": 47,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 14,
          "rejected": 47,
          "approval_rate": 0.2295
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "ds-r1-qwen-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 22,
          "rejected": 39,
          "approval_rate": 0.3607
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 27,
          "rejected": 25,
          "approval_rate": 0.5192
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.4426,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 34,
          "rejected": 23,
          "approval_rate": 0.5965
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 34,
          "rejected": 23,
          "approval_rate": 0.5965
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 11,
          "rate": 0.1803
        },
        "llm_judged": {
          "count": 50,
          "rate": 0.8197,
          "approved": 34,
          "rejected": 16,
          "approval_rate": 0.68
        },
        "accuracy_from_exact_match": 0.1803,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 11,
        "llm_calls": 50,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.4918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 30,
        "incorrect": 31,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 27,
          "rejected": 31,
          "approval_rate": 0.4655
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4426,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 26,
          "rejected": 32,
          "approval_rate": 0.4483
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 30,
          "rejected": 28,
          "approval_rate": 0.5172
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4918,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 29,
          "rejected": 29,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "ds-v3.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 33,
          "rejected": 27,
          "approval_rate": 0.55
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 24,
          "rejected": 27,
          "approval_rate": 0.4706
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 10,
          "rate": 0.1639
        },
        "llm_judged": {
          "count": 51,
          "rate": 0.8361,
          "approved": 25,
          "rejected": 26,
          "approval_rate": 0.4902
        },
        "accuracy_from_exact_match": 0.1639,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 10,
        "llm_calls": 51,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 31,
          "rejected": 26,
          "approval_rate": 0.5439
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 32,
          "rejected": 21,
          "approval_rate": 0.6038
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 14,
          "rejected": 43,
          "approval_rate": 0.2456
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.2623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 16,
        "incorrect": 45,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 13,
          "rejected": 45,
          "approval_rate": 0.2241
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.2623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 16,
        "incorrect": 45,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 15,
          "rejected": 45,
          "approval_rate": 0.25
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "gemma3-27b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 18,
          "rejected": 40,
          "approval_rate": 0.3103
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 33,
          "rejected": 23,
          "approval_rate": 0.5893
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 31,
          "rejected": 25,
          "approval_rate": 0.5536
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.5082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 34,
          "rejected": 23,
          "approval_rate": 0.5965
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 31,
          "rejected": 28,
          "approval_rate": 0.5254
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.5082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 9,
          "rate": 0.1475
        },
        "llm_judged": {
          "count": 52,
          "rate": 0.8525,
          "approved": 38,
          "rejected": 14,
          "approval_rate": 0.7308
        },
        "accuracy_from_exact_match": 0.1475,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 9,
        "llm_calls": 52,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 33,
          "rejected": 26,
          "approval_rate": 0.5593
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 34,
          "rejected": 25,
          "approval_rate": 0.5763
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 35,
          "rejected": 23,
          "approval_rate": 0.6034
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 35,
          "rejected": 26,
          "approval_rate": 0.5738
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "gpt-4.1_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 38,
          "rejected": 21,
          "approval_rate": 0.6441
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 35,
          "rejected": 20,
          "approval_rate": 0.6364
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 7,
          "rate": 0.1148
        },
        "llm_judged": {
          "count": 54,
          "rate": 0.8852,
          "approved": 30,
          "rejected": 24,
          "approval_rate": 0.5556
        },
        "accuracy_from_exact_match": 0.1148,
        "accuracy_boost_from_llm": 0.4918,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 7,
        "llm_calls": 54,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 38,
          "rejected": 19,
          "approval_rate": 0.6667
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 36,
          "rejected": 24,
          "approval_rate": 0.6
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.8197,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 50,
        "incorrect": 11,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 42,
          "rejected": 11,
          "approval_rate": 0.7925
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.6885,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 33,
          "rejected": 27,
          "approval_rate": 0.55
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 35,
          "rejected": 23,
          "approval_rate": 0.6034
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 39,
          "rejected": 21,
          "approval_rate": 0.65
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.6393,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "gpt-5.2_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7705,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 47,
        "incorrect": 14,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 46,
          "rejected": 14,
          "approval_rate": 0.7667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.7541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 36,
          "rejected": 20,
          "approval_rate": 0.6429
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 33,
          "rejected": 23,
          "approval_rate": 0.5893
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.4262,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 26,
        "incorrect": 35,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 23,
          "rejected": 35,
          "approval_rate": 0.3966
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 36,
          "rejected": 17,
          "approval_rate": 0.6792
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.5902,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 16,
          "rejected": 43,
          "approval_rate": 0.2712
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.2787,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 17,
        "incorrect": 44,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 15,
          "rejected": 44,
          "approval_rate": 0.2542
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 21,
          "rejected": 40,
          "approval_rate": 0.3443
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "llama-70b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 18,
          "rejected": 43,
          "approval_rate": 0.2951
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 20,
          "rejected": 36,
          "approval_rate": 0.3571
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 18,
          "rejected": 39,
          "approval_rate": 0.3158
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 21,
          "rejected": 40,
          "approval_rate": 0.3443
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.1475,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 9,
        "incorrect": 52,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 9,
          "rejected": 52,
          "approval_rate": 0.1475
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.1475,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 9,
        "incorrect": 52,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 8,
          "rejected": 52,
          "approval_rate": 0.1333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.1311,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "llama-8b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.1475,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 9,
        "incorrect": 52,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 9,
          "rejected": 52,
          "approval_rate": 0.1475
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 13,
          "rejected": 22,
          "approval_rate": 0.3714
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 10,
          "rejected": 22,
          "approval_rate": 0.3125
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 13,
          "rejected": 22,
          "approval_rate": 0.3714
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.4918,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 30,
        "incorrect": 31,
        "exact_match": {
          "count": 18,
          "rate": 0.2951
        },
        "llm_judged": {
          "count": 43,
          "rate": 0.7049,
          "approved": 12,
          "rejected": 31,
          "approval_rate": 0.2791
        },
        "accuracy_from_exact_match": 0.2951,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 18,
        "llm_calls": 43,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 11,
          "rejected": 19,
          "approval_rate": 0.3667
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 14,
          "rejected": 21,
          "approval_rate": 0.4
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 10,
          "rejected": 24,
          "approval_rate": 0.2941
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 17,
          "rejected": 19,
          "approval_rate": 0.4722
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 31,
        "incorrect": 30,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 10,
          "rejected": 30,
          "approval_rate": 0.25
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "llama-8b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 31,
          "rate": 0.5082
        },
        "llm_judged": {
          "count": 30,
          "rate": 0.4918,
          "approved": 9,
          "rejected": 21,
          "approval_rate": 0.3
        },
        "accuracy_from_exact_match": 0.5082,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 31,
        "llm_calls": 30,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "mistral_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 28,
        "incorrect": 33,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 25,
          "rejected": 33,
          "approval_rate": 0.431
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "mistral_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 26,
          "rejected": 32,
          "approval_rate": 0.4483
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.4262,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "mistral_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 24,
          "rejected": 36,
          "approval_rate": 0.4
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "mistral_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 29,
          "rejected": 32,
          "approval_rate": 0.4754
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.4754,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "mistral_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "mistral_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "mistral_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 5,
        "incorrect": 56,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 5,
          "rejected": 56,
          "approval_rate": 0.082
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 18,
          "rejected": 20,
          "approval_rate": 0.4737
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.2951,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 7,
          "rejected": 22,
          "approval_rate": 0.2414
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1148,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 13,
          "rejected": 27,
          "approval_rate": 0.325
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 20,
          "rate": 0.3279
        },
        "llm_judged": {
          "count": 41,
          "rate": 0.6721,
          "approved": 12,
          "rejected": 29,
          "approval_rate": 0.2927
        },
        "accuracy_from_exact_match": 0.3279,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 20,
        "llm_calls": 41,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 10,
          "rejected": 19,
          "approval_rate": 0.3448
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 15,
          "rejected": 22,
          "approval_rate": 0.4054
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 28,
          "rate": 0.459
        },
        "llm_judged": {
          "count": 33,
          "rate": 0.541,
          "approved": 13,
          "rejected": 20,
          "approval_rate": 0.3939
        },
        "accuracy_from_exact_match": 0.459,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 28,
        "llm_calls": 33,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 15,
          "rejected": 27,
          "approval_rate": 0.3571
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 13,
          "rejected": 27,
          "approval_rate": 0.325
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "mistral_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "mistral_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 11,
          "rejected": 18,
          "approval_rate": 0.3793
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "phi3_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 20,
          "rejected": 40,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "phi3_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 16,
          "rejected": 42,
          "approval_rate": 0.2759
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "phi3_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 17,
          "rejected": 43,
          "approval_rate": 0.2833
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "phi3_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.4098,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 25,
        "incorrect": 36,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 22,
          "rejected": 36,
          "approval_rate": 0.3793
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "phi3_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "phi3_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "phi3_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "phi3_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.0656,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 4,
        "incorrect": 57,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 4,
          "rejected": 57,
          "approval_rate": 0.0656
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0656,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 15,
          "rejected": 27,
          "approval_rate": 0.3571
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 10,
          "rejected": 26,
          "approval_rate": 0.2778
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 16,
          "rejected": 22,
          "approval_rate": 0.4211
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 17,
          "rejected": 29,
          "approval_rate": 0.3696
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 11,
          "rejected": 24,
          "approval_rate": 0.3143
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 15,
          "rejected": 25,
          "approval_rate": 0.375
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 11,
          "rejected": 27,
          "approval_rate": 0.2895
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 11,
          "rejected": 26,
          "approval_rate": 0.2973
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 28,
        "incorrect": 33,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 13,
          "rejected": 33,
          "approval_rate": 0.2826
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "phi3_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "phi3_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 7,
          "rejected": 29,
          "approval_rate": 0.1944
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.1148,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 34,
          "rejected": 23,
          "approval_rate": 0.5965
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 28,
          "rejected": 28,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 33,
          "rejected": 27,
          "approval_rate": 0.55
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.541,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 38,
          "rejected": 17,
          "approval_rate": 0.6909
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2787,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 17,
        "incorrect": 44,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 15,
          "rejected": 44,
          "approval_rate": 0.2542
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.4426,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 27,
        "incorrect": 34,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 25,
          "rejected": 34,
          "approval_rate": 0.4237
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.4098,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 23,
          "rejected": 38,
          "approval_rate": 0.377
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.377,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.5246,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 32,
        "incorrect": 29,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 31,
          "rejected": 29,
          "approval_rate": 0.5167
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.5082,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 21,
          "rejected": 40,
          "approval_rate": 0.3443
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.2787,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 17,
        "incorrect": 44,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 16,
          "rejected": 44,
          "approval_rate": 0.2667
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 20,
          "rejected": 40,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.0164,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 1,
        "incorrect": 60,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 1,
          "rejected": 60,
          "approval_rate": 0.0164
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0164,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.0492,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 3,
        "incorrect": 58,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 3,
          "rejected": 58,
          "approval_rate": 0.0492
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0492,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-3b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.0328,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 2,
        "incorrect": 59,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 2,
          "rejected": 59,
          "approval_rate": 0.0328
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.0328,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 27,
          "rate": 0.4426
        },
        "llm_judged": {
          "count": 34,
          "rate": 0.5574,
          "approved": 13,
          "rejected": 21,
          "approval_rate": 0.3824
        },
        "accuracy_from_exact_match": 0.4426,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 27,
        "llm_calls": 34,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 15,
          "rejected": 20,
          "approval_rate": 0.4286
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.6393,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 39,
        "incorrect": 22,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 16,
          "rejected": 22,
          "approval_rate": 0.4211
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.541,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 33,
        "incorrect": 28,
        "exact_match": {
          "count": 19,
          "rate": 0.3115
        },
        "llm_judged": {
          "count": 42,
          "rate": 0.6885,
          "approved": 14,
          "rejected": 28,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.3115,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 19,
        "llm_calls": 42,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 8,
          "rejected": 24,
          "approval_rate": 0.25
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.1311,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 22,
          "rate": 0.3607
        },
        "llm_judged": {
          "count": 39,
          "rate": 0.6393,
          "approved": 13,
          "rejected": 26,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.3607,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 22,
        "llm_calls": 39,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 25,
          "rate": 0.4098
        },
        "llm_judged": {
          "count": 36,
          "rate": 0.5902,
          "approved": 11,
          "rejected": 25,
          "approval_rate": 0.3056
        },
        "accuracy_from_exact_match": 0.4098,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 25,
        "llm_calls": 36,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 13,
          "rejected": 27,
          "approval_rate": 0.325
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.4754,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 29,
        "incorrect": 32,
        "exact_match": {
          "count": 15,
          "rate": 0.2459
        },
        "llm_judged": {
          "count": 46,
          "rate": 0.7541,
          "approved": 14,
          "rejected": 32,
          "approval_rate": 0.3043
        },
        "accuracy_from_exact_match": 0.2459,
        "accuracy_boost_from_llm": 0.2295,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 15,
        "llm_calls": 46,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-3b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-3b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6066,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 37,
        "incorrect": 24,
        "exact_match": {
          "count": 28,
          "rate": 0.459
        },
        "llm_judged": {
          "count": 33,
          "rate": 0.541,
          "approved": 9,
          "rejected": 24,
          "approval_rate": 0.2727
        },
        "accuracy_from_exact_match": 0.459,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 28,
        "llm_calls": 33,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 5,
          "rate": 0.082
        },
        "llm_judged": {
          "count": 56,
          "rate": 0.918,
          "approved": 17,
          "rejected": 39,
          "approval_rate": 0.3036
        },
        "accuracy_from_exact_match": 0.082,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 5,
        "llm_calls": 56,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.3115,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 19,
        "incorrect": 42,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 15,
          "rejected": 42,
          "approval_rate": 0.2632
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.4262,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 26,
        "incorrect": 35,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 24,
          "rejected": 35,
          "approval_rate": 0.4068
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.3934,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 15,
          "rejected": 43,
          "approval_rate": 0.2586
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.1967,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 12,
        "incorrect": 49,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 12,
          "rejected": 49,
          "approval_rate": 0.1967
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.2459,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 15,
        "incorrect": 46,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 15,
          "rejected": 46,
          "approval_rate": 0.2459
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.1148,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 7,
        "incorrect": 54,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 7,
          "rejected": 54,
          "approval_rate": 0.1148
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1148,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.1639,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 10,
        "incorrect": 51,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 9,
          "rejected": 51,
          "approval_rate": 0.15
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-7b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.1639,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 10,
        "incorrect": 51,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 10,
          "rejected": 51,
          "approval_rate": 0.1639
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed42",
      "metrics": {
        "accuracy": 0.7377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 45,
        "incorrect": 16,
        "exact_match": {
          "count": 29,
          "rate": 0.4754
        },
        "llm_judged": {
          "count": 32,
          "rate": 0.5246,
          "approved": 16,
          "rejected": 16,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.4754,
        "accuracy_boost_from_llm": 0.2623,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 29,
        "llm_calls": 32,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed43",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 9,
          "rejected": 20,
          "approval_rate": 0.3103
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed44",
      "metrics": {
        "accuracy": 0.623,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 38,
        "incorrect": 23,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 17,
          "rejected": 23,
          "approval_rate": 0.425
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2787,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed45",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 24,
          "rate": 0.3934
        },
        "llm_judged": {
          "count": 37,
          "rate": 0.6066,
          "approved": 11,
          "rejected": 26,
          "approval_rate": 0.2973
        },
        "accuracy_from_exact_match": 0.3934,
        "accuracy_boost_from_llm": 0.1803,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 24,
        "llm_calls": 37,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_few_shot_seed46",
      "metrics": {
        "accuracy": 0.6721,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 41,
        "incorrect": 20,
        "exact_match": {
          "count": 35,
          "rate": 0.5738
        },
        "llm_judged": {
          "count": 26,
          "rate": 0.4262,
          "approved": 6,
          "rejected": 20,
          "approval_rate": 0.2308
        },
        "accuracy_from_exact_match": 0.5738,
        "accuracy_boost_from_llm": 0.0984,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 35,
        "llm_calls": 26,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.7213,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 44,
        "incorrect": 17,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 12,
          "rejected": 17,
          "approval_rate": 0.4138
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1967,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.5738,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 35,
        "incorrect": 26,
        "exact_match": {
          "count": 26,
          "rate": 0.4262
        },
        "llm_judged": {
          "count": 35,
          "rate": 0.5738,
          "approved": 9,
          "rejected": 26,
          "approval_rate": 0.2571
        },
        "accuracy_from_exact_match": 0.4262,
        "accuracy_boost_from_llm": 0.1475,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 26,
        "llm_calls": 35,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed44",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 21,
          "rate": 0.3443
        },
        "llm_judged": {
          "count": 40,
          "rate": 0.6557,
          "approved": 13,
          "rejected": 27,
          "approval_rate": 0.325
        },
        "accuracy_from_exact_match": 0.3443,
        "accuracy_boost_from_llm": 0.2131,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 21,
        "llm_calls": 40,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.5082,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 31,
        "incorrect": 30,
        "exact_match": {
          "count": 23,
          "rate": 0.377
        },
        "llm_judged": {
          "count": 38,
          "rate": 0.623,
          "approved": 8,
          "rejected": 30,
          "approval_rate": 0.2105
        },
        "accuracy_from_exact_match": 0.377,
        "accuracy_boost_from_llm": 0.1311,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 23,
        "llm_calls": 38,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-7b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-7b_finetuned_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.6885,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 42,
        "incorrect": 19,
        "exact_match": {
          "count": 32,
          "rate": 0.5246
        },
        "llm_judged": {
          "count": 29,
          "rate": 0.4754,
          "approved": 10,
          "rejected": 19,
          "approval_rate": 0.3448
        },
        "accuracy_from_exact_match": 0.5246,
        "accuracy_boost_from_llm": 0.1639,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 32,
        "llm_calls": 29,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed42",
      "metrics": {
        "accuracy": 0.6557,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 40,
        "incorrect": 21,
        "exact_match": {
          "count": 6,
          "rate": 0.0984
        },
        "llm_judged": {
          "count": 55,
          "rate": 0.9016,
          "approved": 34,
          "rejected": 21,
          "approval_rate": 0.6182
        },
        "accuracy_from_exact_match": 0.0984,
        "accuracy_boost_from_llm": 0.5574,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 6,
        "llm_calls": 55,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed43",
      "metrics": {
        "accuracy": 0.5574,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 34,
        "incorrect": 27,
        "exact_match": {
          "count": 7,
          "rate": 0.1148
        },
        "llm_judged": {
          "count": 54,
          "rate": 0.8852,
          "approved": 27,
          "rejected": 27,
          "approval_rate": 0.5
        },
        "accuracy_from_exact_match": 0.1148,
        "accuracy_boost_from_llm": 0.4426,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 7,
        "llm_calls": 54,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed44",
      "metrics": {
        "accuracy": 0.5902,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 36,
        "incorrect": 25,
        "exact_match": {
          "count": 4,
          "rate": 0.0656
        },
        "llm_judged": {
          "count": 57,
          "rate": 0.9344,
          "approved": 32,
          "rejected": 25,
          "approval_rate": 0.5614
        },
        "accuracy_from_exact_match": 0.0656,
        "accuracy_boost_from_llm": 0.5246,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 4,
        "llm_calls": 57,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_few_shot_seed46",
      "metrics": {
        "accuracy": 0.7049,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 43,
        "incorrect": 18,
        "exact_match": {
          "count": 8,
          "rate": 0.1311
        },
        "llm_judged": {
          "count": 53,
          "rate": 0.8689,
          "approved": 35,
          "rejected": 18,
          "approval_rate": 0.6604
        },
        "accuracy_from_exact_match": 0.1311,
        "accuracy_boost_from_llm": 0.5738,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 8,
        "llm_calls": 53,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed42",
      "metrics": {
        "accuracy": 0.2951,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 18,
        "incorrect": 43,
        "exact_match": {
          "count": 3,
          "rate": 0.0492
        },
        "llm_judged": {
          "count": 58,
          "rate": 0.9508,
          "approved": 15,
          "rejected": 43,
          "approval_rate": 0.2586
        },
        "accuracy_from_exact_match": 0.0492,
        "accuracy_boost_from_llm": 0.2459,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 3,
        "llm_calls": 58,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed43",
      "metrics": {
        "accuracy": 0.377,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 23,
        "incorrect": 38,
        "exact_match": {
          "count": 2,
          "rate": 0.0328
        },
        "llm_judged": {
          "count": 59,
          "rate": 0.9672,
          "approved": 21,
          "rejected": 38,
          "approval_rate": 0.3559
        },
        "accuracy_from_exact_match": 0.0328,
        "accuracy_boost_from_llm": 0.3443,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 2,
        "llm_calls": 59,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed45",
      "metrics": {
        "accuracy": 0.3443,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 21,
        "incorrect": 40,
        "exact_match": {
          "count": 1,
          "rate": 0.0164
        },
        "llm_judged": {
          "count": 60,
          "rate": 0.9836,
          "approved": 20,
          "rejected": 40,
          "approval_rate": 0.3333
        },
        "accuracy_from_exact_match": 0.0164,
        "accuracy_boost_from_llm": 0.3279,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 1,
        "llm_calls": 60,
        "no_llm": 0
      }
    },
    {
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "stem": "qwen-coder-32b_baseline_zero_shot_seed46",
      "metrics": {
        "accuracy": 0.3607,
        "total_evaluated": 61,
        "evaluated": 61,
        "correct": 22,
        "incorrect": 39,
        "exact_match": {
          "count": 0,
          "rate": 0.0
        },
        "llm_judged": {
          "count": 61,
          "rate": 1.0,
          "approved": 22,
          "rejected": 39,
          "approval_rate": 0.3607
        },
        "accuracy_from_exact_match": 0.0,
        "accuracy_boost_from_llm": 0.3607,
        "no_llm_fallback_count": 0
      },
      "stats": {
        "unmatched": 0,
        "auto_exact": 0,
        "llm_calls": 61,
        "no_llm": 0
      }
    }
  ],
  "ranking": [
    {
      "rank": 1,
      "source_file": "gpt-5.2_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.8197,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.7925,
      "total": 61
    },
    {
      "rank": 2,
      "source_file": "gpt-4.1_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.7308,
      "total": 61
    },
    {
      "rank": 3,
      "source_file": "gpt-5.2_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7705,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.7667,
      "total": 61
    },
    {
      "rank": 4,
      "source_file": "ds-v3.2_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.1803,
      "llm_approval_rate": 0.68,
      "total": 61
    },
    {
      "rank": 5,
      "source_file": "qwen-7b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.7377,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 6,
      "source_file": "llama-70b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.6792,
      "total": 61
    },
    {
      "rank": 7,
      "source_file": "qwen-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.6909,
      "total": 61
    },
    {
      "rank": 8,
      "source_file": "qwen-7b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.7213,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.4138,
      "total": 61
    },
    {
      "rank": 9,
      "source_file": "mistral_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.3793,
      "total": 61
    },
    {
      "rank": 10,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.7049,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.6604,
      "total": 61
    },
    {
      "rank": 11,
      "source_file": "gpt-5.2_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.6667,
      "total": 61
    },
    {
      "rank": 12,
      "source_file": "llama-8b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.3667,
      "total": 61
    },
    {
      "rank": 13,
      "source_file": "llama-8b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.4722,
      "total": 61
    },
    {
      "rank": 14,
      "source_file": "mistral_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.3448,
      "total": 61
    },
    {
      "rank": 15,
      "source_file": "qwen-7b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6885,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.3448,
      "total": 61
    },
    {
      "rank": 16,
      "source_file": "gpt-5.2_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.6364,
      "total": 61
    },
    {
      "rank": 17,
      "source_file": "llama-70b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.6429,
      "total": 61
    },
    {
      "rank": 18,
      "source_file": "mistral_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.4737,
      "total": 61
    },
    {
      "rank": 19,
      "source_file": "mistral_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.459,
      "llm_approval_rate": 0.3939,
      "total": 61
    },
    {
      "rank": 20,
      "source_file": "qwen-3b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.4286,
      "total": 61
    },
    {
      "rank": 21,
      "source_file": "qwen-7b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.3103,
      "total": 61
    },
    {
      "rank": 22,
      "source_file": "qwen-7b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6721,
      "exact_match_rate": 0.5738,
      "llm_approval_rate": 0.2308,
      "total": 61
    },
    {
      "rank": 23,
      "source_file": "gemma3-27b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.1311,
      "llm_approval_rate": 0.6038,
      "total": 61
    },
    {
      "rank": 24,
      "source_file": "gpt-4.1_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.6441,
      "total": 61
    },
    {
      "rank": 25,
      "source_file": "gpt-5.2_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.65,
      "total": 61
    },
    {
      "rank": 26,
      "source_file": "llama-8b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.4,
      "total": 61
    },
    {
      "rank": 27,
      "source_file": "llama-8b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.5082,
      "llm_approval_rate": 0.3,
      "total": 61
    },
    {
      "rank": 28,
      "source_file": "qwen-3b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.3824,
      "total": 61
    },
    {
      "rank": 29,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6557,
      "exact_match_rate": 0.0984,
      "llm_approval_rate": 0.6182,
      "total": 61
    },
    {
      "rank": 30,
      "source_file": "llama-8b_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.3714,
      "total": 61
    },
    {
      "rank": 31,
      "source_file": "llama-8b_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.3125,
      "total": 61
    },
    {
      "rank": 32,
      "source_file": "llama-8b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.3714,
      "total": 61
    },
    {
      "rank": 33,
      "source_file": "mistral_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.5246,
      "llm_approval_rate": 0.2414,
      "total": 61
    },
    {
      "rank": 34,
      "source_file": "mistral_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.4054,
      "total": 61
    },
    {
      "rank": 35,
      "source_file": "phi3_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.4211,
      "total": 61
    },
    {
      "rank": 36,
      "source_file": "qwen-3b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.6393,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.4211,
      "total": 61
    },
    {
      "rank": 37,
      "source_file": "ds-v3.2_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5965,
      "total": 61
    },
    {
      "rank": 38,
      "source_file": "ds-v3.2_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5965,
      "total": 61
    },
    {
      "rank": 39,
      "source_file": "gpt-4.1_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5893,
      "total": 61
    },
    {
      "rank": 40,
      "source_file": "gpt-4.1_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5965,
      "total": 61
    },
    {
      "rank": 41,
      "source_file": "gpt-4.1_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6034,
      "total": 61
    },
    {
      "rank": 42,
      "source_file": "gpt-5.2_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.6034,
      "total": 61
    },
    {
      "rank": 43,
      "source_file": "llama-70b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5893,
      "total": 61
    },
    {
      "rank": 44,
      "source_file": "qwen-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5965,
      "total": 61
    },
    {
      "rank": 45,
      "source_file": "qwen-7b_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.623,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.425,
      "total": 61
    },
    {
      "rank": 46,
      "source_file": "gpt-5.2_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.1148,
      "llm_approval_rate": 0.5556,
      "total": 61
    },
    {
      "rank": 47,
      "source_file": "gpt-5.2_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.6,
      "total": 61
    },
    {
      "rank": 48,
      "source_file": "llama-8b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.4426,
      "llm_approval_rate": 0.2941,
      "total": 61
    },
    {
      "rank": 49,
      "source_file": "phi3_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.3143,
      "total": 61
    },
    {
      "rank": 50,
      "source_file": "qwen-3b_finetuned_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.4754,
      "llm_approval_rate": 0.25,
      "total": 61
    },
    {
      "rank": 51,
      "source_file": "qwen-3b_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.6066,
      "exact_match_rate": 0.459,
      "llm_approval_rate": 0.2727,
      "total": 61
    },
    {
      "rank": 52,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5614,
      "total": 61
    },
    {
      "rank": 53,
      "source_file": "ds-v3.2_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.1475,
      "llm_approval_rate": 0.5192,
      "total": 61
    },
    {
      "rank": 54,
      "source_file": "gpt-4.1_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5536,
      "total": 61
    },
    {
      "rank": 55,
      "source_file": "gpt-4.1_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.5763,
      "total": 61
    },
    {
      "rank": 56,
      "source_file": "phi3_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.375,
      "total": 61
    },
    {
      "rank": 57,
      "source_file": "qwen-3b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.3056,
      "total": 61
    },
    {
      "rank": 58,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5902,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5614,
      "total": 61
    },
    {
      "rank": 59,
      "source_file": "gemma3-27b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.4902,
      "total": 61
    },
    {
      "rank": 60,
      "source_file": "gemma3-27b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.5439,
      "total": 61
    },
    {
      "rank": 61,
      "source_file": "gpt-4.1_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.5593,
      "total": 61
    },
    {
      "rank": 62,
      "source_file": "gpt-4.1_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.5738,
      "total": 61
    },
    {
      "rank": 63,
      "source_file": "phi3_finetuned_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.2778,
      "total": 61
    },
    {
      "rank": 64,
      "source_file": "phi3_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.2973,
      "total": 61
    },
    {
      "rank": 65,
      "source_file": "qwen-3b_finetuned_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.3607,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 66,
      "source_file": "qwen-7b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.3934,
      "llm_approval_rate": 0.2973,
      "total": 61
    },
    {
      "rank": 67,
      "source_file": "qwen-7b_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5738,
      "exact_match_rate": 0.4262,
      "llm_approval_rate": 0.2571,
      "total": 61
    },
    {
      "rank": 68,
      "source_file": "ds-v3.2_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.55,
      "total": 61
    },
    {
      "rank": 69,
      "source_file": "gemma3-27b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.1639,
      "llm_approval_rate": 0.4706,
      "total": 61
    },
    {
      "rank": 70,
      "source_file": "gpt-5.2_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.55,
      "total": 61
    },
    {
      "rank": 71,
      "source_file": "mistral_finetuned_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.325,
      "total": 61
    },
    {
      "rank": 72,
      "source_file": "mistral_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.3571,
      "total": 61
    },
    {
      "rank": 73,
      "source_file": "mistral_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.325,
      "total": 61
    },
    {
      "rank": 74,
      "source_file": "phi3_finetuned_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.3571,
      "total": 61
    },
    {
      "rank": 75,
      "source_file": "phi3_finetuned_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.2895,
      "total": 61
    },
    {
      "rank": 76,
      "source_file": "qwen-32b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.55,
      "total": 61
    },
    {
      "rank": 77,
      "source_file": "qwen-3b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.325,
      "total": 61
    },
    {
      "rank": 78,
      "source_file": "qwen-7b_finetuned_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.325,
      "total": 61
    },
    {
      "rank": 79,
      "source_file": "qwen-coder-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5574,
      "exact_match_rate": 0.1148,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 80,
      "source_file": "ds-v3.2_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5172,
      "total": 61
    },
    {
      "rank": 81,
      "source_file": "gpt-4.1_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.5254,
      "total": 61
    },
    {
      "rank": 82,
      "source_file": "qwen-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 83,
      "source_file": "qwen-3b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.541,
      "exact_match_rate": 0.3115,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 84,
      "source_file": "ds-v3.2_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.5,
      "total": 61
    },
    {
      "rank": 85,
      "source_file": "mistral_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.3279,
      "llm_approval_rate": 0.2927,
      "total": 61
    },
    {
      "rank": 86,
      "source_file": "phi3_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.3696,
      "total": 61
    },
    {
      "rank": 87,
      "source_file": "phi3_finetuned_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.4098,
      "llm_approval_rate": 0.1944,
      "total": 61
    },
    {
      "rank": 88,
      "source_file": "qwen-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.5246,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.5167,
      "total": 61
    },
    {
      "rank": 89,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.5082,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4915,
      "total": 61
    },
    {
      "rank": 90,
      "source_file": "ds-r1-qwen-32b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.5082,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.4643,
      "total": 61
    },
    {
      "rank": 91,
      "source_file": "llama-8b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5082,
      "exact_match_rate": 0.3443,
      "llm_approval_rate": 0.25,
      "total": 61
    },
    {
      "rank": 92,
      "source_file": "qwen-7b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.5082,
      "exact_match_rate": 0.377,
      "llm_approval_rate": 0.2105,
      "total": 61
    },
    {
      "rank": 93,
      "source_file": "ds-v3.2_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.4918,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.4655,
      "total": 61
    },
    {
      "rank": 94,
      "source_file": "llama-8b_finetuned_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.4918,
      "exact_match_rate": 0.2951,
      "llm_approval_rate": 0.2791,
      "total": 61
    },
    {
      "rank": 95,
      "source_file": "ds-v3.2_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.4483,
      "total": 61
    },
    {
      "rank": 96,
      "source_file": "mistral_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.4483,
      "total": 61
    },
    {
      "rank": 97,
      "source_file": "mistral_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.4754,
      "total": 61
    },
    {
      "rank": 98,
      "source_file": "qwen-3b_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.4754,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.3043,
      "total": 61
    },
    {
      "rank": 99,
      "source_file": "mistral_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.459,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.431,
      "total": 61
    },
    {
      "rank": 100,
      "source_file": "phi3_finetuned_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.459,
      "exact_match_rate": 0.2459,
      "llm_approval_rate": 0.2826,
      "total": 61
    },
    {
      "rank": 101,
      "source_file": "qwen-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.4426,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4237,
      "total": 61
    },
    {
      "rank": 102,
      "source_file": "llama-70b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.4262,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3966,
      "total": 61
    },
    {
      "rank": 103,
      "source_file": "qwen-7b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.4262,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.4068,
      "total": 61
    },
    {
      "rank": 104,
      "source_file": "llama-8b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.3571,
      "total": 61
    },
    {
      "rank": 105,
      "source_file": "mistral_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.4,
      "total": 61
    },
    {
      "rank": 106,
      "source_file": "phi3_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.4098,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3793,
      "total": 61
    },
    {
      "rank": 107,
      "source_file": "qwen-32b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.377,
      "total": 61
    },
    {
      "rank": 108,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.377,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.3559,
      "total": 61
    },
    {
      "rank": 109,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3276,
      "total": 61
    },
    {
      "rank": 110,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3607,
      "total": 61
    },
    {
      "rank": 111,
      "source_file": "llama-8b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.3158,
      "total": 61
    },
    {
      "rank": 112,
      "source_file": "qwen-7b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.082,
      "llm_approval_rate": 0.3036,
      "total": 61
    },
    {
      "rank": 113,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.3607,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3607,
      "total": 61
    },
    {
      "rank": 114,
      "source_file": "gemma3-27b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.3103,
      "total": 61
    },
    {
      "rank": 115,
      "source_file": "llama-70b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3443,
      "total": 61
    },
    {
      "rank": 116,
      "source_file": "llama-8b_baseline_few_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3443,
      "total": 61
    },
    {
      "rank": 117,
      "source_file": "phi3_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 118,
      "source_file": "qwen-3b_baseline_few_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.3443,
      "total": 61
    },
    {
      "rank": 119,
      "source_file": "qwen-3b_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 120,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.3443,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.3333,
      "total": 61
    },
    {
      "rank": 121,
      "source_file": "phi3_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.2759,
      "total": 61
    },
    {
      "rank": 122,
      "source_file": "qwen-7b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.3115,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.2632,
      "total": 61
    },
    {
      "rank": 123,
      "source_file": "gemma3-27b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0656,
      "llm_approval_rate": 0.2456,
      "total": 61
    },
    {
      "rank": 124,
      "source_file": "llama-70b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.2712,
      "total": 61
    },
    {
      "rank": 125,
      "source_file": "llama-70b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2951,
      "total": 61
    },
    {
      "rank": 126,
      "source_file": "phi3_baseline_few_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.2833,
      "total": 61
    },
    {
      "rank": 127,
      "source_file": "qwen-7b_baseline_few_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.2586,
      "total": 61
    },
    {
      "rank": 128,
      "source_file": "qwen-coder-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.2951,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.2586,
      "total": 61
    },
    {
      "rank": 129,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.2787,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.2542,
      "total": 61
    },
    {
      "rank": 130,
      "source_file": "llama-70b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.2787,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.2542,
      "total": 61
    },
    {
      "rank": 131,
      "source_file": "qwen-32b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.2787,
      "exact_match_rate": 0.0328,
      "llm_approval_rate": 0.2542,
      "total": 61
    },
    {
      "rank": 132,
      "source_file": "qwen-3b_baseline_few_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.2787,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.2667,
      "total": 61
    },
    {
      "rank": 133,
      "source_file": "gemma3-27b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.2623,
      "exact_match_rate": 0.0492,
      "llm_approval_rate": 0.2241,
      "total": 61
    },
    {
      "rank": 134,
      "source_file": "gemma3-27b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.2623,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.25,
      "total": 61
    },
    {
      "rank": 135,
      "source_file": "qwen-7b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.2459,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2459,
      "total": 61
    },
    {
      "rank": 136,
      "source_file": "ds-r1-qwen-32b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.2295,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.2295,
      "total": 61
    },
    {
      "rank": 137,
      "source_file": "qwen-7b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.1967,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1967,
      "total": 61
    },
    {
      "rank": 138,
      "source_file": "qwen-7b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.1639,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.15,
      "total": 61
    },
    {
      "rank": 139,
      "source_file": "qwen-7b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.1639,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1639,
      "total": 61
    },
    {
      "rank": 140,
      "source_file": "llama-8b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.1475,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1475,
      "total": 61
    },
    {
      "rank": 141,
      "source_file": "llama-8b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.1475,
      "exact_match_rate": 0.0164,
      "llm_approval_rate": 0.1333,
      "total": 61
    },
    {
      "rank": 142,
      "source_file": "llama-8b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.1475,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1475,
      "total": 61
    },
    {
      "rank": 143,
      "source_file": "qwen-7b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.1148,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.1148,
      "total": 61
    },
    {
      "rank": 144,
      "source_file": "mistral_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.082,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.082,
      "total": 61
    },
    {
      "rank": 145,
      "source_file": "llama-8b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 146,
      "source_file": "mistral_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 147,
      "source_file": "phi3_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 148,
      "source_file": "phi3_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.0656,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0656,
      "total": 61
    },
    {
      "rank": 149,
      "source_file": "mistral_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 150,
      "source_file": "phi3_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 151,
      "source_file": "qwen-3b_baseline_zero_shot_seed44__judge-gpt-5.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 152,
      "source_file": "qwen-3b_baseline_zero_shot_seed45__judge-gpt-5.2.json",
      "accuracy": 0.0492,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0492,
      "total": 61
    },
    {
      "rank": 153,
      "source_file": "phi3_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 154,
      "source_file": "qwen-3b_baseline_zero_shot_seed42__judge-gpt-5.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 155,
      "source_file": "qwen-3b_baseline_zero_shot_seed46__judge-gpt-5.2.json",
      "accuracy": 0.0328,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0328,
      "total": 61
    },
    {
      "rank": 156,
      "source_file": "qwen-3b_baseline_zero_shot_seed43__judge-gpt-5.2.json",
      "accuracy": 0.0164,
      "exact_match_rate": 0.0,
      "llm_approval_rate": 0.0164,
      "total": 61
    }
  ],
  "totals": {
    "evaluated": 9516,
    "auto_exact": 1523,
    "llm_calls": 7993,
    "no_llm": 0
  }
}